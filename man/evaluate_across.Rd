% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate-across.R
\name{evaluate_across}
\alias{evaluate_across}
\title{Evaluate LLM performance across models/prompts/etc}
\usage{
evaluate_across(path = ".", across = tibble(), repeats = 1L, ...)
}
\arguments{
\item{path}{Path to the directory or file in question. Optional.}

\item{across}{A data frame where each column represents an option to be set
when evaluating the file at \code{path} and each row represents a pass through
that file.}

\item{repeats}{A single positive integer specifying the number of
evaluation repeats, or runs over the same test files. Assuming that the
models you're evaluating provide non-deterministic output, running the
same test files multiple times by setting \code{repeats > 1} will help you
quantify the variability of your evaluations.}

\item{...}{Additional arguments passed to internal functions.}
}
\description{
\code{evaluate_across()} is an extension of \code{\link[=evaluate]{evaluate()}} and friends that
allows users to run evals across various combinations of models, prompts,
or any other arbitrary parameter.
}
\examples{
\dontshow{if (FALSE) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
# evaluate a directory of evals across several models,
# repeating each eval twice
eval <- evaluate_across(
  "tests/evalthat/test-ggplot2.R",
  across = tibble(chat = c(
    chat_openai(model = "gpt-4o-mini", echo = FALSE),
    chat_claude(model = "claude-3-5-sonnet-latest", echo = FALSE))
  ),
  repeats = 2
)
\dontshow{\}) # examplesIf}
}
