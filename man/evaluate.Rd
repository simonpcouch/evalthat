% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate.R
\name{evaluate}
\alias{evaluate}
\alias{evaluate_active_file}
\title{Evaluate LLM performance}
\usage{
evaluate(path = ".", repeats = 1L, ...)

evaluate_active_file(path = active_eval_file(), repeats = 1L, ...)
}
\arguments{
\item{path}{Path to the directory or file in question. Optional.}

\item{repeats}{A single positive integer specifying the number of
evaluation repeats, or runs over the same test files. Assuming that the
models you're evaluating provide non-deterministic output, running the
same test files multiple times by setting \code{repeats > 1} will help you
quantify the variability of your evaluations.}

\item{...}{Additional arguments passed to internal functions.}
}
\value{
Results of the evaluation, invisibly. Evaluation results contain information
on the eval metadata as well as numbers of failures
and passes, input and output, and descriptions of each failure.

The function also has side-effects:
\itemize{
\item An interactive progress interface tracking results in real-time.
\item \emph{Result files} are stored in \verb{dirname(path)/_results}. Result files contain
persistent, fine-grained evaluation results and can be interfaced with
via \code{\link[=results_read]{results_read()}} and friends.
}
}
\description{
\code{evaluate()} and \code{evaluate_active_file()} are roughly analogous to
\code{\link[devtools:test]{devtools::test()}} and \code{\link[devtools:test]{devtools::test_active_file()}}, though note that
\code{evaluate()} can take either a directory of files or a single file.
}
