[{"path":"https://simonpcouch.github.io/evalthat/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Posit Software, PBC Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/evalthat/articles/model-grading.html","id":"models-should-not-judge-their-own-output","dir":"Articles","previous_headings":"","what":"Models should not judge their own output","title":"Model grading","text":"Ergonomically, ’s totally reasonable practitioner want use model generate answers use judge . ’ve gone effort set API key configured access model, can’t just use judge responses? unfortunately case; LLMs prone self-enhancement bias, ’re likely prefer answer one supplied another model. holds even models don’t know given response arises (Ye et al. 2024). , evalthat exclude models evaluations responses generated default.","code":""},{"path":"https://simonpcouch.github.io/evalthat/articles/model-grading.html","id":"judge-using-strong-llms","dir":"Articles","previous_headings":"","what":"Judge using “strong” LLMs","title":"Model grading","text":"reduce compute associated evaluating, many studies proposed making use smaller models fine-tuned specifically judging. models shown promise applications (Verga et al. 2024), research generally shown larger models intended broader use cases tend make better evaluators; “although fine-tuned judge models achieve high performance -domain test sets, even surpassing GPT-4, underperform GPT-4 across several dimensions, including generalizability, fairness, aspect-specific evaluation, scalability” (Fu et al. 2024; Huang et al. 2024). Colloquially, models often referred “strong” LLMs (Zheng et al. 2023; Gu et al. 2025). said, several findings cited based results utilizing—least part—smaller, open-source models judges (Fu et al. 2024; Schroeder Wood-Doughty 2024).","code":""},{"path":"https://simonpcouch.github.io/evalthat/articles/model-grading.html","id":"prefer-pairwise-comparisons-over-scoring","dir":"Articles","previous_headings":"","what":"Prefer pairwise comparisons over scoring","title":"Model grading","text":"LLMs often used evaluate output two notable ways: Pairwise comparisons: Two models asked question (possibly human-written reference answer) provide answers. , third model provided question, desired answer, two model responses, asked choose one two model responses preference. Scoring: model asked question (possibly human-written reference answer) provides answer. , another model provided question, desired answer, model’s response, possibly along rubric, asked rate response according rubric numeric scale. Scoring methods shown easily influenced connotations wordings unrelated pieces context, models shown evaluate reliably comparing pairwise (Ouyang et al. 2022; Schroeder Wood-Doughty 2024).","code":""},{"path":"https://simonpcouch.github.io/evalthat/articles/model-grading.html","id":"position-matters","dir":"Articles","previous_headings":"Prefer pairwise comparisons over scoring","what":"Position matters","title":"Model grading","text":"context pairwise comparisons, many language models vulnerable position bias, models tend prefer response presented first one presented second (vice versa) regardless quality response (Wang et al. 2023). models much resilient others—example, GPT-4-turbo shown prefer response order swapped 80.5% time, LLaMA3-8B-Instruct 38.9% time (Gu et al. 2025). Generally, strong LLMs tend less susceptible bias variety methods exist address .","code":""},{"path":"https://simonpcouch.github.io/evalthat/articles/model-grading.html","id":"run-the-same-evaluation-multiple-times","dir":"Articles","previous_headings":"","what":"Run the same evaluation multiple times","title":"Model grading","text":"Notably, related addressing position bias, summarizing evaluations across multiple runs content shown lead better evaluations. mitigate position bias, Zheng et al. (2023) call judge twice “swapping order two answers declare win answer preferred orders.” preferences inconsistent swapping orders, call tie. context scoring, Wang et al. (2023) polling judge orders taking average score. studies proposed aggregating evaluations across several separate models can mitigate biases. example, many models demonstrate verbosity bias, tend rate longer responses highly. However, Ye et al. (2024) showed “response length influences model judgment complex ways;” models much less susceptible bias others, even penalize excessively verbose answers. similar story goes compassion fade, models might prefer responses containing references positively connoted words, attentional bias, models might disproportionately incorporate irrelevant information evaluation (Koo et al. 2024). fact models vary susceptibility biases means aggregating responses across many often outperforms use single judge (Verga et al. 2024; Schroeder Wood-Doughty 2024). Gu et al. (2025) show case majority voting specifically.","code":""},{"path":"https://simonpcouch.github.io/evalthat/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Couch. Author, maintainer.","code":""},{"path":"https://simonpcouch.github.io/evalthat/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Couch S (2025). evalthat: LLM Evaluation R. R package version 0.0.0.9000, https://simonpcouch.github.io/evalthat/, https://github.com/simonpcouch/evalthat.","code":"@Manual{,   title = {evalthat: LLM Evaluation in R},   author = {Simon Couch},   year = {2025},   note = {R package version 0.0.0.9000, https://simonpcouch.github.io/evalthat/},   url = {https://github.com/simonpcouch/evalthat}, }"},{"path":"https://simonpcouch.github.io/evalthat/index.html","id":"evalthat","dir":"","previous_headings":"","what":"LLM Evaluation in R","title":"LLM Evaluation in R","text":"evalthat provides testthat-style framework LLM evaluation R. can write unit tests, can compare performance across various LLMs, improve prompts using evidence, quantify variability model output. Caution! package early development much documentation aspirational.","code":""},{"path":"https://simonpcouch.github.io/evalthat/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"LLM Evaluation in R","text":"can install development version evalthat like :","code":"# install.packages(\"pak\") pak::pak(\"simonpcouch/evalthat\")"},{"path":"https://simonpcouch.github.io/evalthat/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"LLM Evaluation in R","text":"evalthat code looks lot like testthat code. ’s example: testthat users notice couple changes: testing file wrapped function() arguments defining variables evaluate across. Unlike typical test files, evals meant ran across many configurations help users compare prompts, model providers, etc. functions input() output() flag “went model?” “came ?” addition regular expect_*() functions testthat, package supplies number new expectation functions helpful evaluating R code contained character string (outputted ellmer extensions). begin expect_*() automated, begin grade_*() less-. Running test file results persistent result file—think like snapshot. evalthat supplies number helpers working result files, allowing compare performance across various models, iterate prompts, quantify variability output, . full ggplot2 example file, run 5 passes evaluating several different models revising ggplot2 code:  Evaluation functions return data frame information evaluation results analysis: Visualizing example output:","code":"function(chat = ellmer::chat_claude()) {   test_that(\"model can make a basic histogram\", {     input <- input(       \"Write ggplot code to plot a histogram of the mpg variable in mtcars.         Return only the plotting code, no backticks and no exposition.\"     )          output <- output(chat$chat(input))          # check that output was syntactically code R code     expect_r_code(output)          # match keywords to affirm intended functionality     expect_match(output, \"ggplot(\", fixed = TRUE)     expect_match(output, \"aes(\", fixed = TRUE)     expect_match(output, \"geom_histogram(\", fixed = TRUE)          # flag output for grading either by yourself or an LLM judge     target <- \"ggplot(mtcars) + aes(x = mpg) + geom_histogram()\"     grade_output(target)   }) } library(ellmer)  eval <- evaluate(   \"tests/evalthat/test-ggplot2.R\",   across = tibble(chat = c(     chat_openai(model = \"gpt-4o\", echo = FALSE),     chat_openai(model = \"gpt-4o-mini\", echo = FALSE),     chat_claude(model = \"claude-3-5-sonnet-latest\", echo = FALSE),     chat_ollama(model = \"qwen2.5-coder:14b\", echo = FALSE)   )),   repeats = 5 ) eval #> # A tibble: 20 × 8 #>    chat              pct n_fail n_pass timestamp file_hash io           problems #>    <chr>           <dbl>  <dbl>  <dbl> <chr>     <chr>     <list>       <list>   #>  1 ollama qwen2.5…  94.4      4     67 20250115… a801f5a8… <named list> <list>   #>  2 ollama qwen2.5…  85.9     10     61 20250115… a801f5a8… <named list> <list>   #>  3 ollama qwen2.5…  91.5      6     65 20250115… a801f5a8… <named list> <list>   #>  4 ollama qwen2.5…  88.7      8     63 20250115… a801f5a8… <named list> <list>   #>  5 ollama qwen2.5…  83.1     12     59 20250115… a801f5a8… <named list> <list>   #>  6 Claude claude-… 100        0     71 20250115… a801f5a8… <named list> <list>   #>  7 Claude claude-…  97.2      2     69 20250115… a801f5a8… <named list> <list>   #>  8 Claude claude-… 100        0     71 20250115… a801f5a8… <named list> <list>   #>  9 Claude claude-…  98.6      1     70 20250115… a801f5a8… <named list> <list>   #> 10 Claude claude-… 100        0     71 20250115… a801f5a8… <named list> <list>   #> 11 OpenAI gpt-4o-…  95.8      3     68 20250115… a801f5a8… <named list> <list>   #> 12 OpenAI gpt-4o-…  94.4      4     67 20250115… a801f5a8… <named list> <list>   #> 13 OpenAI gpt-4o-…  90.1      7     64 20250115… a801f5a8… <named list> <list>   #> 14 OpenAI gpt-4o-…  94.4      4     67 20250115… a801f5a8… <named list> <list>   #> 15 OpenAI gpt-4o-…  94.4      4     67 20250115… a801f5a8… <named list> <list>   #> 16 OpenAI gpt-4o    95.8      3     68 20250115… a801f5a8… <named list> <list>   #> 17 OpenAI gpt-4o    93.0      5     66 20250115… a801f5a8… <named list> <list>   #> 18 OpenAI gpt-4o    95.8      3     68 20250115… a801f5a8… <named list> <list>   #> 19 OpenAI gpt-4o    94.4      4     67 20250115… a801f5a8… <named list> <list>   #> 20 OpenAI gpt-4o    97.2      2     69 20250115… a801f5a8… <named list> <list>"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":null,"dir":"Reference","previous_headings":"","what":"Test reporters for LLM evaluation — EvalReporters","title":"Test reporters for LLM evaluation — EvalReporters","text":"evalthat provides number custom testthat testthat::Reporters. reporters process results test files, generating interactive summaries saving results persistent files. EvalProgressReporter designed interactive use. goal give actionable insights help understand status code. reporter also praises time--time tests pass. default reporter test_dir(). EvalCompactProgressReporter minimal version EvalProgressReporter designed use single files. default reporter test_file().","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Test reporters for LLM evaluation — EvalReporters","text":"testthat::Reporter -> testthat::ProgressReporter -> EvalProgressReporter","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Test reporters for LLM evaluation — EvalReporters","text":"show_praise Whether show praise. shown, even TRUE. res_ok, res_skip, res_warn, res_fail fields prefixed n_* superclass, encoded vector whose sum can taken find n_* values. Enables weights per-expectation. ctxt_res_ok, ctxt_res_skip, ctxt_res_warn, ctxt_res_fail, ctxt_issues, ctxt_n without ctxt_* prefix, per-context. io inputs outputs flagged input() output().","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Test reporters for LLM evaluation — EvalReporters","text":"res_ok, res_skip, res_warn, res_fail fields prefixed n_* superclass, encoded vector whose sum can taken find n_* values. Enables weights per-expectation. ctxt_res_ok, ctxt_res_skip, ctxt_res_warn, ctxt_res_fail, ctxt_issues, ctxt_n without ctxt_* prefix, per-context.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Test reporters for LLM evaluation — EvalReporters","text":"testthat::Reporter$.start_context() testthat::Reporter$cat_line() testthat::Reporter$cat_tight() testthat::Reporter$end_context_if_started() testthat::Reporter$end_test() testthat::Reporter$local_user_output() testthat::Reporter$rule() testthat::Reporter$start_test() testthat::Reporter$update() testthat::ProgressReporter$cr() testthat::ProgressReporter$initialize() testthat::ProgressReporter$report_issues() testthat::ProgressReporter$should_update() testthat::ProgressReporter$status_data()","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Test reporters for LLM evaluation — EvalReporters","text":"EvalProgressReporter$is_full() EvalProgressReporter$update_io() EvalProgressReporter$start_file() EvalProgressReporter$update_counts() EvalProgressReporter$reset_counts() EvalProgressReporter$start_context() EvalProgressReporter$show_header() EvalProgressReporter$show_status() EvalProgressReporter$end_context() EvalProgressReporter$save_results() EvalProgressReporter$result_summary() EvalProgressReporter$add_result() EvalProgressReporter$cache_for_grading() EvalProgressReporter$start_reporter() EvalProgressReporter$end_reporter() EvalProgressReporter$end_file() EvalProgressReporter$clone()","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-is-full-","dir":"Reference","previous_headings":"","what":"Method is_full()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Method overwritting always return FALSE, failed evals reason stop testing context.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$is_full()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-update-io-","dir":"Reference","previous_headings":"","what":"Method update_io()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Concatenated inputs outputs flagged input() output(). Concatenate inputs outputs flagged input() output().","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$update_io(x, test, type)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"x input output. test name test chunk. type One \"input\" \"output\".","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-start-file-","dir":"Reference","previous_headings":"","what":"Method start_file()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Setup starting new test file.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$start_file(file)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"file File name.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-update-counts-","dir":"Reference","previous_headings":"","what":"Method update_counts()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Eval reporters' n_* fields actually tracked maintaining vectors 1s prefixed res_*. accommodate future extension weights can assigned pass/fail. function updates n_* values based res_* values.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$update_counts()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-reset-counts-","dir":"Reference","previous_headings":"","what":"Method reset_counts()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Sets values res_* fields numeric() calls update_counts().","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$reset_counts()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-start-context-","dir":"Reference","previous_headings":"","what":"Method start_context()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Resets counters initiates progress bar.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$start_context(context)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"context Arguments supplied evaluation function, named list.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-show-header-","dir":"Reference","previous_headings":"","what":"Method show_header()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Show header tabulating successes failures.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$show_header()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-show-status-","dir":"Reference","previous_headings":"","what":"Method show_status()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Tabulate successes failures current context.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$show_status(complete = FALSE, time = 0, pad = FALSE)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"complete Logical. time Ignored. TODO: remove ? pad Logical.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-end-context-","dir":"Reference","previous_headings":"","what":"Method end_context()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Teardown following test run.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$end_context()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-save-results-","dir":"Reference","previous_headings":"","what":"Method save_results()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Situates evaluation results tibble saves file eval_file_name/timestamp.rds using qs::qread(). Read individual results qs::qsave(). Situate evaluation results tibble.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$save_results()"},{"path":[]},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$result_summary(timestamp)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"timestamp DTTM format(Sys.time(), \"%Y%m%d_%H%M%S\").","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-add-result-","dir":"Reference","previous_headings":"","what":"Method add_result()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Append given expect_*() grade_*() result current context.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$add_result(context, test, result)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"context Context evaluating(). test name test block. result Result.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$cache_for_grading(target)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-start-reporter-","dir":"Reference","previous_headings":"","what":"Method start_reporter()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Start current reporter.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$start_reporter()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-end-reporter-","dir":"Reference","previous_headings":"","what":"Method end_reporter()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Tear current reporter.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$end_reporter()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-end-file-","dir":"Reference","previous_headings":"","what":"Method end_file()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Overrides superclass' method end context file ends since files reran running evals.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$end_file()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Test reporters for LLM evaluation — EvalReporters","text":"objects class cloneable method.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalProgressReporter$clone(deep = FALSE)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"deep Whether make deep clone.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"super-classes-1","dir":"Reference","previous_headings":"","what":"Super classes","title":"Test reporters for LLM evaluation — EvalReporters","text":"testthat::Reporter -> testthat::ProgressReporter -> evalthat::EvalProgressReporter -> EvalCompactProgressReporter","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"methods-1","dir":"Reference","previous_headings":"","what":"Methods","title":"Test reporters for LLM evaluation — EvalReporters","text":"testthat::Reporter$.start_context() testthat::Reporter$cat_line() testthat::Reporter$cat_tight() testthat::Reporter$end_context_if_started() testthat::Reporter$end_test() testthat::Reporter$local_user_output() testthat::Reporter$rule() testthat::Reporter$start_test() testthat::Reporter$update() testthat::ProgressReporter$cr() testthat::ProgressReporter$report_issues() testthat::ProgressReporter$should_update() testthat::ProgressReporter$status_data() evalthat::EvalProgressReporter$add_result() evalthat::EvalProgressReporter$cache_for_grading() evalthat::EvalProgressReporter$end_context() evalthat::EvalProgressReporter$end_file() evalthat::EvalProgressReporter$is_full() evalthat::EvalProgressReporter$reset_counts() evalthat::EvalProgressReporter$result_summary() evalthat::EvalProgressReporter$save_results() evalthat::EvalProgressReporter$show_header() evalthat::EvalProgressReporter$start_context() evalthat::EvalProgressReporter$update_counts() evalthat::EvalProgressReporter$update_io()","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"public-methods-1","dir":"Reference","previous_headings":"","what":"Public methods","title":"Test reporters for LLM evaluation — EvalReporters","text":"EvalCompactProgressReporter$new() EvalCompactProgressReporter$start_file() EvalCompactProgressReporter$start_reporter() EvalCompactProgressReporter$end_reporter() EvalCompactProgressReporter$show_status() EvalCompactProgressReporter$clone()","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Sets minimum time infinity.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalCompactProgressReporter$new(min_time = Inf, ...)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"min_time numeric. Defaults Inf. ... Passed super$initialize().","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-start-file--1","dir":"Reference","previous_headings":"","what":"Method start_file()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Setup single file.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalCompactProgressReporter$start_file(name)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"name File name.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-start-reporter--1","dir":"Reference","previous_headings":"","what":"Method start_reporter()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Setup.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalCompactProgressReporter$start_reporter()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-end-reporter--1","dir":"Reference","previous_headings":"","what":"Method end_reporter()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Teardown.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalCompactProgressReporter$end_reporter()"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-show-status--1","dir":"Reference","previous_headings":"","what":"Method show_status()","title":"Test reporters for LLM evaluation — EvalReporters","text":"Show current status.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalCompactProgressReporter$show_status(complete = NULL, time = NULL)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"complete Ignored. time Ignored–included compatibility EvalProgressReporter.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"method-clone--1","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Test reporters for LLM evaluation — EvalReporters","text":"objects class cloneable method.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"Test reporters for LLM evaluation — EvalReporters","text":"","code":"EvalCompactProgressReporter$clone(deep = FALSE)"},{"path":"https://simonpcouch.github.io/evalthat/reference/EvalReporters.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test reporters for LLM evaluation — EvalReporters","text":"deep Whether make deep clone.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/evalthat-package.html","id":null,"dir":"Reference","previous_headings":"","what":"evalthat: LLM Evaluation in R — evalthat-package","title":"evalthat: LLM Evaluation in R — evalthat-package","text":"Provides testthat-style utilities evaluating large language models R. Score models non-deterministic output using unit test interface.","code":""},{"path":[]},{"path":"https://simonpcouch.github.io/evalthat/reference/evalthat-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"evalthat: LLM Evaluation in R — evalthat-package","text":"Maintainer: Simon Couch simon.couch@posit.co (ORCID)","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/evaluate.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate LLM performance — evaluate","title":"Evaluate LLM performance — evaluate","text":"evaluate() evaluate_active_file() roughly analogous devtools::test() devtools::test_active_file(), though note evaluate() can take either directory files single file.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/evaluate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate LLM performance — evaluate","text":"","code":"evaluate(path = \".\", across = tibble(), repeats = 1L, ...)  evaluate_active_file(   path = active_eval_file(),   across = tibble(),   repeats = 1L,   ... )"},{"path":"https://simonpcouch.github.io/evalthat/reference/evaluate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate LLM performance — evaluate","text":"path Path directory file containing evaluation code. across data frame column represents option set evaluating file path row represents pass file. repeats single positive integer specifying number evaluation repeats, runs test files. Assuming models evaluating provide non-deterministic output, running test files multiple times setting repeats > 1 help quantify variability evaluations. ... Additional arguments passed internal functions.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/evaluate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate LLM performance — evaluate","text":"Results evaluation, invisibly. Evaluation results contain information eval metadata well numbers failures passes, input output, descriptions failure. function also side-effects: interactive progress interface tracking results real-time. Result files stored dirname(path)/_results. Result files contain persistent, fine-grained evaluation results can interfaced via results_read() friends.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/evaluate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate LLM performance — evaluate","text":"","code":"if (FALSE) { library(ellmer)  # evaluate with the default model twice evaluate(\"tests/evalthat/test-ggplot2.R\", repeats = 2)  # evaluate a directory of evals across several models, # repeating each eval twice eval <- evaluate(   \"tests/evalthat/test-ggplot2.R\",   across = tibble(chat = c(     chat_openai(model = \"gpt-4o-mini\", echo = FALSE),     chat_claude(model = \"claude-3-5-sonnet-latest\", echo = FALSE))   ),   repeats = 2 ) }"},{"path":"https://simonpcouch.github.io/evalthat/reference/expect_r_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if input is syntactically valid R code — expect_r_code","title":"Check if input is syntactically valid R code — expect_r_code","text":"Check input syntactically valid R code","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/expect_r_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if input is syntactically valid R code — expect_r_code","text":"","code":"expect_r_code(object)"},{"path":"https://simonpcouch.github.io/evalthat/reference/expect_r_code.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if input is syntactically valid R code — expect_r_code","text":"object single string, possibly containing parse-able R code.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/expect_r_code.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if input is syntactically valid R code — expect_r_code","text":"Returns nothing throws error input valid R code.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/expect_r_code.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check if input is syntactically valid R code — expect_r_code","text":"helper attempt evaluate code provided string. Instead, uses treesitter::parser_parse() determine whether string contains valid R syntax. expectation fail object contains exposition surrounding code, including code surrounded backticks.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/grade_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Grade model outputs — grade_output","title":"Grade model outputs — grade_output","text":"Grade model outputs","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/grade_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grade model outputs — grade_output","text":"","code":"grade_output(target)"},{"path":"https://simonpcouch.github.io/evalthat/reference/grade_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grade model outputs — grade_output","text":"target single string describing desired output. Set NULL omit desired output, though discouraged unless grading output .","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/grade_queue.html","id":null,"dir":"Reference","previous_headings":"","what":"Grade an evaluation data frame — grade_queue","title":"Grade an evaluation data frame — grade_queue","text":"set judges argument non-NULL value evaluate(), function evoked automatically.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/grade_queue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grade an evaluation data frame — grade_queue","text":"","code":"grade_queue(x, judges, type = c(\"pairwise\", \"score\"))"},{"path":"https://simonpcouch.github.io/evalthat/reference/grade_queue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grade an evaluation data frame — grade_queue","text":"judges judges() object NULL grade evals . type One \"pairwise\" \"score\". See details .","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/grade_queue.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Grade an evaluation data frame — grade_queue","text":"","code":"library(ellmer)  ggplot2 <- evaluate(   \"tests/evalthat/test-ggplot2-graded.R\",     across = tibble(chat = c(         chat_openai(model = \"gpt-4o-mini\", echo = FALSE),         chat_claude(model = \"claude-3-5-sonnet-latest\", echo = FALSE),         chat_ollama(model = \"qwen2.5-coder:14b\", echo = FALSE))       ),       repeats = 2     ) #> Error in tibble(chat = c(chat_openai(model = \"gpt-4o-mini\", echo = FALSE),     chat_claude(model = \"claude-3-5-sonnet-latest\", echo = FALSE),     chat_ollama(model = \"qwen2.5-coder:14b\", echo = FALSE))): could not find function \"tibble\"  ggplot2 #> Error: object 'ggplot2' not found  ggplot2_graded <- grade_queue(   ggplot2,   judges = judges(gpt4o = chat_openai(model = \"gpt-4o\")) ) #> Error in grade_queue(ggplot2, judges = judges(gpt4o = chat_openai(model = \"gpt-4o\"))): could not find function \"grade_queue\"  ggplot2_graded #> Error: object 'ggplot2_graded' not found"},{"path":"https://simonpcouch.github.io/evalthat/reference/io.html","id":null,"dir":"Reference","previous_headings":"","what":"Flag model inputs and outputs — io","title":"Flag model inputs and outputs — io","text":"evaluating model input evaluations, flag input input() output output(), allowing persistent logging responses.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/io.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flag model inputs and outputs — io","text":"","code":"input(x)  output(x)"},{"path":"https://simonpcouch.github.io/evalthat/reference/io.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flag model inputs and outputs — io","text":"x character string representing either model input output.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/io.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Flag model inputs and outputs — io","text":"x, provided. Called side effect.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/io.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Flag model inputs and outputs — io","text":"","code":"if (FALSE) { test_that(\"the model works\", {   input <- input(\"some input\")   output <- output(some_function_call_generating_response(input)) }) }"},{"path":"https://simonpcouch.github.io/evalthat/reference/judges.html","id":null,"dir":"Reference","previous_headings":"","what":"Define judge models — judges","title":"Define judge models — judges","text":"Judge models allow automated grading model output (grade_model()) passing responses LLM judges. learn evalthat makes use judge models, see vignette(\"Model grading\", package = \"evalthat\").","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/judges.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define judge models — judges","text":"","code":"judges(...)"},{"path":"https://simonpcouch.github.io/evalthat/reference/judges.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define judge models — judges","text":"... Named (optionally) ellmer::Chat() objects, e.g. output ellmer::chat_openai() ellmer::chat_claude().","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/judges.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define judge models — judges","text":"","code":"library(ellmer)  j <- judges(   gpt4o = chat_openai(model = \"gpt-4o\"),   claude = chat_claude() ) #> Error in openai_key(): Can't find env var `OPENAI_API_KEY`.  j #> Error: object 'j' not found  # doesn't necessarily need to be named: judges(chat_openai(model = \"gpt-4o\"), chat_claude()) #> Error in openai_key(): Can't find env var `OPENAI_API_KEY`."},{"path":"https://simonpcouch.github.io/evalthat/reference/results_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Interface with eval results — results_read","title":"Interface with eval results — results_read","text":"Interface eval results","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/results_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interface with eval results — results_read","text":"","code":"results_read(dir = NULL)"},{"path":"https://simonpcouch.github.io/evalthat/reference/results_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interface with eval results — results_read","text":"dir Optional. single string specifying subdirectory tests/evalthat/_results. NULL, read subdirectory row-bind results.","code":""},{"path":"https://simonpcouch.github.io/evalthat/reference/results_read.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interface with eval results — results_read","text":"tibble containing combined results result files specified directory. Evaluations ordered chronologically, recent results top.","code":""}]
