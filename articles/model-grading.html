<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Model grading • evalthat</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Model grading">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">evalthat</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/model-grading.html">Model grading</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/simonpcouch/evalthat/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Model grading</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/simonpcouch/evalthat/blob/main/vignettes/model-grading.Rmd" class="external-link"><code>vignettes/model-grading.Rmd</code></a></small>
      <div class="d-none name"><code>model-grading.Rmd</code></div>
    </div>

    
    
<p>Among other things, the evalthat package provides a suite of
functionality for <strong>model grading</strong> or, as it’s often
referred to in the literature, <strong>LLM-as-a-judge</strong>. Model
grading entails an LLM evaluating a response from an LLM. That is, after
asking a question to an LLM and receiving an answer, both the question
and answer are provided to another language model and that model is
asked to somehow judge whether the provided response was
satisfactory.</p>
<p>Given the infamously stochastic and gullible tendencies of LLMs, it’s
not all obvious that this should work. Indeed, many earlier attempts at
such a framework were unsuccessful by most measures, and only recently
have sufficient advancements been made to the point where model grading
is a helpful tool in an evaluation toolkit. <!--TODO: more here --></p>
<p>The design of evalthat’s model grading tools is heavily influenced by
the most recent research on LLM-as-a-judge. This vignette will outline
some of the findings that guided evalthat’s interface for model
grading.</p>
<div class="section level3">
<h3 id="section"></h3>
</div>
<div class="section level2">
<h2 id="models-should-not-judge-their-own-output">Models should not judge their own output<a class="anchor" aria-label="anchor" href="#models-should-not-judge-their-own-output"></a>
</h2>
<p>Ergonomically, it’s totally reasonable that a practitioner would want
to use the same model to generate answers as they use to judge them. If
I’ve gone to the effort to set up an API key and configured access to a
model, can’t I just use it to judge its own responses? This is
unfortunately not the case; LLMs are prone to <strong>self-enhancement
bias</strong>, where they’re likely to prefer their own answer over one
supplied by another model. This holds up even when models don’t know
where a given response arises from <span class="citation">(Ye et al.
2024)</span>. As such, evalthat will exclude models from evaluations of
responses they generated themselves by default.</p>
</div>
<div class="section level2">
<h2 id="judge-using-strong-llms">Judge using “strong” LLMs<a class="anchor" aria-label="anchor" href="#judge-using-strong-llms"></a>
</h2>
<p>To reduce the compute associated with evaluating, many studies have
proposed making use of smaller models fine-tuned specifically for
judging. While these models have shown promise in some applications
<span class="citation">(Verga et al. 2024)</span>, research has
generally shown that larger models intended for broader use cases tend
to make for better evaluators; “although the fine-tuned judge models
achieve high performance on in-domain test sets, even surpassing GPT-4,
they underperform GPT-4 across several dimensions, including
generalizability, fairness, aspect-specific evaluation, and scalability”
<span class="citation">(Fu et al. 2024; Huang et al. 2024)</span>.
Colloquially, such models are often referred to as “strong” LLMs <span class="citation">(Zheng et al. 2023; Gu et al. 2025)</span>.</p>
<p>That said, several of the findings cited here are based on results
utilizing—at least in part—smaller, open-source models as judges <span class="citation">(Fu et al. 2024; Schroeder and Wood-Doughty
2024)</span>.</p>
</div>
<div class="section level2">
<h2 id="prefer-pairwise-comparisons-over-scoring">Prefer pairwise comparisons over scoring<a class="anchor" aria-label="anchor" href="#prefer-pairwise-comparisons-over-scoring"></a>
</h2>
<p>LLMs are often used to evaluate output in two notable ways:</p>
<ul>
<li><p>Pairwise comparisons: Two models are asked a question (possibly
with a human-written reference answer) and both provide answers. Then, a
third model is provided the question, the desired answer, and the two
model responses, and is asked to choose one of the two model responses
as its preference.</p></li>
<li><p>Scoring: A model is asked a question (possibly with a
human-written reference answer) and provides an answer. Then, another
model is provided the question, the desired answer, and the model’s
response, possibly along with a rubric, and is asked to rate the
response according to the rubric on some numeric scale.</p></li>
</ul>
<p>Scoring methods have been shown to be easily influenced by
connotations of wordings and unrelated pieces of context, while models
have been shown to evaluate more reliably when comparing pairwise <span class="citation">(Ouyang et al. 2022; Schroeder and Wood-Doughty
2024)</span>.</p>
<div class="section level3">
<h3 id="position-matters">Position matters<a class="anchor" aria-label="anchor" href="#position-matters"></a>
</h3>
<p>In the context of pairwise comparisons, many language models are
vulnerable to <strong>position bias</strong>, where models will tend to
prefer the response presented first over the one presented second (or
vice versa) regardless of the quality of the response <span class="citation">(Wang et al. 2023)</span>. Some models are much more
resilient to this than others—for example, <em>GPT-4-turbo</em> was
shown to prefer the same response when the order was swapped 80.5% of
the time, while <em>LLaMA3-8B-Instruct</em> did so 38.9% of the time
<span class="citation">(Gu et al. 2025)</span>. Generally, strong LLMs
tend to be less susceptible to this bias and a variety of methods exist
to address it.
<!--# Notably, swapping the order of responses and aggregating across evaluations has been shown to be effective[@zheng2023judgingllmasajudgemtbenchchatbot; @wang2023largelanguagemodelsfair]. --></p>
</div>
</div>
<div class="section level2">
<h2 id="run-the-same-evaluation-multiple-times">Run the same evaluation multiple times<a class="anchor" aria-label="anchor" href="#run-the-same-evaluation-multiple-times"></a>
</h2>
<p>Notably, related to addressing position bias, summarizing evaluations
across multiple runs on the same content has been shown to lead to
better evaluations.</p>
<p>To mitigate position bias, <span class="citation">Zheng et al.
(2023)</span> call the same judge twice “by swapping the order of two
answers and only declare a win when an answer is preferred in both
orders.” When the preferences are inconsistent after swapping orders,
they call it a tie. In the context of scoring, <span class="citation">Wang et al. (2023)</span> do so by polling the judge
with both orders and then taking the average of the score.</p>
<p>Other studies have proposed that aggregating evaluations across
several separate models can mitigate <em>other</em> biases. For example,
many models demonstrate <strong>verbosity bias</strong>, where they tend
to rate longer responses more highly. However, <span class="citation">Ye
et al. (2024)</span> showed that “response length influences model
judgment in complex ways;” some models are much less susceptible to this
bias than others, and some even penalize excessively verbose answers. A
similar story goes for <strong>compassion fade</strong>, where models
might prefer responses containing references to positively connoted
words, or <strong>attentional bias</strong>, where models might
disproportionately incorporate irrelevant information into their
evaluation <span class="citation">(Koo et al. 2024)</span>. The fact
that models vary in their susceptibility to these biases means that
aggregating responses across many of them often outperforms use of a
single judge <span class="citation">(Verga et al. 2024; Schroeder and
Wood-Doughty 2024)</span>. <span class="citation">Gu et al.
(2025)</span> show this to be the case with majority voting
specifically.</p>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-fu-etal-2024-gptscore" class="csl-entry">
Fu, Jinlan, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2024.
<span>“<span>GPTS</span>core: Evaluate as You Desire.”</span> In
<em>Proceedings of the 2024 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language
Technologies (Volume 1: Long Papers)</em>, edited by Kevin Duh, Helena
Gomez, and Steven Bethard, 6556–76. Mexico City, Mexico: Association for
Computational Linguistics. <a href="https://doi.org/10.18653/v1/2024.naacl-long.365" class="external-link">https://doi.org/10.18653/v1/2024.naacl-long.365</a>.
</div>
<div id="ref-gu2025surveyllmasajudge" class="csl-entry">
Gu, Jiawei, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin
Xu, Wei Li, et al. 2025. <span>“A Survey on LLM-as-a-Judge.”</span> <a href="https://arxiv.org/abs/2411.15594" class="external-link">https://arxiv.org/abs/2411.15594</a>.
</div>
<div id="ref-huang2024empiricalstudyllmasajudgellm" class="csl-entry">
Huang, Hui, Yingqi Qu, Xingyuan Bu, Hongli Zhou, Jing Liu, Muyun Yang,
Bing Xu, and Tiejun Zhao. 2024. <span>“An Empirical Study of
LLM-as-a-Judge for LLM Evaluation: Fine-Tuned Judge Model Is Not a
General Substitute for GPT-4.”</span> <a href="https://arxiv.org/abs/2403.02839" class="external-link">https://arxiv.org/abs/2403.02839</a>.
</div>
<div id="ref-koo2024benchmarkingcognitivebiaseslarge" class="csl-entry">
Koo, Ryan, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and
Dongyeop Kang. 2024. <span>“Benchmarking Cognitive Biases in Large
Language Models as Evaluators.”</span> <a href="https://arxiv.org/abs/2309.17012" class="external-link">https://arxiv.org/abs/2309.17012</a>.
</div>
<div id="ref-ouyang2022training" class="csl-entry">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language
Models to Follow Instructions with Human Feedback.”</span> <em>Advances
in Neural Information Processing Systems</em> 35: 27730–44.
</div>
<div id="ref-schroeder2024trustllmjudgmentsreliability" class="csl-entry">
Schroeder, Kayla, and Zach Wood-Doughty. 2024. <span>“Can You Trust LLM
Judgments? Reliability of LLM-as-a-Judge.”</span> <a href="https://arxiv.org/abs/2412.12509" class="external-link">https://arxiv.org/abs/2412.12509</a>.
</div>
<div id="ref-verga2024replacingjudgesjuriesevaluating" class="csl-entry">
Verga, Pat, Sebastian Hofstatter, Sophia Althammer, Yixuan Su,
Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and
Patrick Lewis. 2024. <span>“Replacing Judges with Juries: Evaluating LLM
Generations with a Panel of Diverse Models.”</span> <a href="https://arxiv.org/abs/2404.18796" class="external-link">https://arxiv.org/abs/2404.18796</a>.
</div>
<div id="ref-wang2023largelanguagemodelsfair" class="csl-entry">
Wang, Peiyi, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin,
Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. <span>“Large
Language Models Are Not Fair Evaluators.”</span> <a href="https://arxiv.org/abs/2305.17926" class="external-link">https://arxiv.org/abs/2305.17926</a>.
</div>
<div id="ref-ye2024justiceprejudicequantifyingbiases" class="csl-entry">
Ye, Jiayi, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno
Moniz, Tian Gao, et al. 2024. <span>“Justice or Prejudice? Quantifying
Biases in LLM-as-a-Judge.”</span> <a href="https://arxiv.org/abs/2410.02736" class="external-link">https://arxiv.org/abs/2410.02736</a>.
</div>
<div id="ref-zheng2023judgingllmasajudgemtbenchchatbot" class="csl-entry">
Zheng, Lianmin, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
Yonghao Zhuang, Zi Lin, et al. 2023. <span>“Judging LLM-as-a-Judge with
MT-Bench and Chatbot Arena.”</span> <a href="https://arxiv.org/abs/2306.05685" class="external-link">https://arxiv.org/abs/2306.05685</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Simon Couch.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
